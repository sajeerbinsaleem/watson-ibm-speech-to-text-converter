<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Watson Speech to Text client example</title>
  <link rel="stylesheet" href="style.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>

<body>
  <div class="container">
    <a href="/">&laquo; Examples</a>

    <section>
      <h2>Transcribe from Microphone with auto stop</h2>
      <button id="button">Start Microphone Transcription</button>

      <h2>Output:</h2>
      <div id="waveform">Waveform</div>
      <div id="output">Text</div>
    </section>

    <script src="scripts/wavesurfer/dist/wavesurfer.min.js"></script>
    <script src="scripts/wavesurfer/dist/wavesurfer.microphone.min.js"></script>
    <script src="scripts/watson-speech/dist/watson-speech.js"></script>
    <!-- window.fetch pollyfill for IE/Edge & Older Chrome/FireFox -->
    <script src="scripts/fetch/dist/fetch.umd.js"></script>

    <h2>Code for this demo:</h2>

    <pre><code><script style="display: block;">

      var speechToken;
      var wavesurfer;

      document.querySelector('#button').onclick = function () {
        fetch('/api/speech-to-text/token')
          .then(function (response) {
            return response.json();
          }).then(function (token) {
            speechToken = token;
            wavesurfer = initMicrophoneVisualizer();
            wavesurfer.microphone.start();
          }).catch(function (error) {
            console.log(error);
          });
      };

      function isSafari() {
        return /^((?!chrome|android).)*safari/i.test(navigator.userAgent || '') || /iPad|iPhone|iPod/i.test(navigator.userAgent || '');
      }

      function initMicrophoneVisualizer() {

        if (wavesurfer === undefined) {

          var context, processor;
          var bufferSize = 4096;

          if (!isSafari())
            barHeight = 2;
          else {
            barHeight = 8;
            bufferSize = 1024;

            context = new (window.AudioContext || window.webkitAudioContext)();
            processor = context.createScriptProcessor(bufferSize, 1, 1);
          }

          wavesurfer = WaveSurfer.create({
            container: '#waveform',
            waveColor: 'red',
            height: 100,
            interact: false,
            cursorWidth: 0,
            barHeight: barHeight,
            barRadius: 18,
            audioContext: context || null,
            audioScriptProcessor: processor || null,
            plugins: [
              WaveSurfer.microphone.create({
                bufferSize: bufferSize,
                numberOfInputChannels: 1,
                numberOfOutputChannels: 1,
                constraints: {
                  video: false,
                  audio: true
                }
              })
            ]
          });

          console.log('Microphone Initialized');

          wavesurfer.microphone.on('deviceReady', function (stream) {
            console.log('Microphone Ready');
            console.log('Wavesurfer Stream: ' + stream.id);
            initiateSpeechVoiceEngine(stream);
          });
          wavesurfer.microphone.on('deviceError', function (code) {
          });
          wavesurfer.on('error', function (e) {
          });
        }

        return wavesurfer;
      }

      function initiateSpeechVoiceEngine(mediaStream) {

        stream = WatsonSpeech.SpeechToText.recognizeMicrophone(Object.assign(speechToken, {
          mediaStream: mediaStream,
          outputElement: '#output'
        }));

        console.log('SpeechToText initiliazed');

        console.log('API Stream: ' + stream.options.mediaStream.id);

        stream.on('data', function (data) {
          if (data.results[0] && data.results[0].final) {
            stream.stop();
            wavesurfer.microphone.stop();
            console.log('SpeechToText stoped.');
          }
        });

        stream.on('error', function (err) {
          console.log(err);
        });
      }
    </script></code></pre>

  </div>
</body>

</html>
